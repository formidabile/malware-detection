import numpy as np
import pandas as pd
import os
import csv
from sklearn.model_selection import GridSearchCV
from pathlib import Path
from tqdm import tqdm
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

LEG_FOLDER = 'C:\\Users\\ivan\\Desktop\\coursework\\legitimate'
VIRUS_FOLDER = 'C:\\Users\\ivan\\Desktop\\coursework\\virus'
TEST_FOLDER = 'C:\\Users\\ivan\\Desktop\\coursework\\test'
REPORT_FOLDER = 'C:\\Users\\ivan\\Desktop\\coursework\\report'

def insert(df, row):
    insert_loc = df.index.max()
    if pd.isna(insert_loc):
        df.loc[0] = row
    else:
        df.loc[insert_loc + 1] = row

def frequency(folder_path, is_zero=True):
    column_names = list(range(256))
    df = pd.DataFrame(columns=column_names)
    files = Path(folder_path).rglob("*")
    for file in tqdm(files):
        if os.path.isdir(file):
            continue
        count = 0.0
        bytes_arr = [0] * 256
        with open(file, "rb") as f:
            text = f.read()
            count += len(text)
            for byte in text:
                bytes_arr[byte] += 1
        insert(df, bytes_arr)
    if not is_zero:
        df.drop(columns=[0], inplace=True)
    return df

def idf(word, df):
    lendf = len(df)
    sumdf = df[word] > 0
    arr = np.log2(len(df) / sum(df[word] > 0))
    return arr

def all_features_without_zero(folder_path, malware_class):
    column_names = list(range(256)) + ['entropy', 'class']
    df = pd.DataFrame(columns=column_names)
    files = Path(folder_path).rglob("*")
    names = []
    for file in tqdm(files):
        if os.path.isdir(file):
            continue
        count = 0.0
        bytes_arr = [0] * 256
        names.append(str(file))
        with open(file, "rb") as f:
            text = f.read()
            count += len(text)
            for byte in text:
                bytes_arr[byte] += 1
            count -= bytes_arr[0]
            entropy = 0
            for i in range(256):
                bytes_arr[i] /= count
                if bytes_arr[i] and i > 0:
                    entropy -= bytes_arr[i] * np.log2(bytes_arr[i])
        insert(df, bytes_arr + [entropy, malware_class])
    df.drop(columns=[0], inplace=True)
    return df, names

def get_x_all_features():
    leg_features, names_leg = all_features_without_zero(LEG_FOLDER, 0)
    virus_features, names_malware = all_features_without_zero(VIRUS_FOLDER, 1)
    test_features, names_test = all_features_without_zero(TEST_FOLDER, 2)
    X = pd.concat([leg_features, virus_features, test_features], ignore_index=True)
    X.columns = X.columns.astype(str)
    names = [names_leg, names_malware, names_test]
    freq_df_leg = frequency(LEG_FOLDER, is_zero=False)
    freq_df_mal = frequency(VIRUS_FOLDER, is_zero=False)
    freq_df_test = frequency(TEST_FOLDER, is_zero=False)
    freq_df = pd.concat([freq_df_leg, freq_df_mal, freq_df_test], ignore_index=True)
    poss_range = range(1, 256)
    column_names = list(poss_range)
    append_str = "tfidf"
    pre_res = [append_str + str(sub) for sub in column_names]
    df = pd.DataFrame(columns=pre_res)
    idf_list = []
    for i in poss_range:
        idf_list.append(idf(i, freq_df))
    for i in range(len(X)):
        tfidf_list = np.array(X.loc[i,])[:-2] * idf_list
    for i in range(0, X.shape[0]):
        insert(df, tfidf_list)
    return pd.concat([X, df], axis=1), names

def proc_data():
    print('Data processing .. Please, wait ')
    X_res, names = get_x_all_features()
    print('Data processing.. DONE')
    X_test = X_res[X_res['class'] == 2]
    X_test.pop('class')
    X_train = X_res[(X_res['class'] == 0) | (X_res['class'] == 1)]
    y_train = X_train.pop('class')
    print(X_train, y_train)
    print('Selection of informative features.. Please, wait')
    pipeline = Pipeline([
         ('scaler', StandardScaler()),
         ('model', Lasso())
    ])
    search = GridSearchCV(pipeline,
                          {'model__alpha': np.arange(0.1, 10, 0.1)},
                          cv=5, scoring="neg_mean_squared_error")
    search.fit(X_train, y_train)
    coefficients = search.best_estimator_.named_steps['model'].coef_
    importance = np.abs(coefficients)
    print('Chosen features: ', np.array(X_train.columns)[importance > 0])
    f_inform = np.array(X_train.columns)[importance > 0]
    X_train = X_train[f_inform]
    X_test = X_test[f_inform]
    print('Selection of informative features.. DONE')
    report = f'''Информативные признаки: {f_inform}'''
    print('Model training .. Please, wait')
    model1 = LogisticRegression(C=10, penalty='l2')
    model2 = RandomForestClassifier(max_depth=7, max_features=4, n_estimators=4)
    print('Model training .. DONE')
    model1.fit(X_train, y_train)
    model2.fit(X_train, y_train)
    predict1 = model1.predict_proba(X_test)
    predict2 = model2.predict_proba(X_test)
    i = 1
    with open('report_1.csv', mode='w') as file:
        writer = csv.writer(file, delimiter=';', lineterminator='\r')
        writer.writerow(['Number', 'File', 'Legitimate_prob', 'Malware_prob', 'Total'])
        for name, (prob0, prob1) in zip(names[2], predict1):
            writer.writerow([i, name, prob0:.3f, prob1:.3f, ans])
            i = i + 1
        for name, (prob0, prob1) in zip(names[2], predict2):
            writer.writerow([i, name, prob0:.3f, prob1:.3f, ans])
            i = i + 1
    print('Report saved in a folder ' + REPORT_FOLDER)

if __name__ == '__main__':
    proc_data()
